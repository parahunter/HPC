\section{Compiler parameters}

//TODO Alexander
 
\section{Comparison against dgemm}

Against the dgemm library function, even with the same compiling options, the implementation provided of the matmult\_nat() method has been proven poor, according to the graph shown in Figure \ref{fig:natdgemmcomp}. The figure shown that, for every possible permutation of the cycle order (including so the naive implementation), dgemm outnumbers it.

//TODO figure

The reason for that ... (investigation required)

\section{Permutations of m, k, n and performance}
The three cycles performed in the naive algorithm can be rearrenged in six different ways: there are in fact six possible permutations of the three letters. The performance results provided by the execution of the six algorithms on some standards memory footprints are provided in Figure \ref{fig:permutations}. The test was made on the product of square matrices $nxnxn$, where $n$ was calculated using the following formula, to relate it to the memory footprint:
$$
n = \sqrt{\frac{F_{byte}}{8 \cdot 3} }
$$

//TODO figure

The best results of the graph are the ones where the most internal cycle is cycled over $[1,n]$, and in particular the best implementation has been proved matmult\_mkn, followed by kmn. The worst ones, instead, have been proved nkm and knm, the ones with the most internal cycle in m.

The explanation of this behaviour is that mkn is the best implementation because is the one that causes the less cache misses. If the most internal variable looped is n, in fact, each time we start reading an element from matrices B and C, that have both n columns, all the row is read. This causes less misses in the cache, leading to a better perfomance for both the mkn and kmn bheviours. Since the A matrix has k columns, also if we loop through k before looping though m we got a better performance, for the same reason. This explains why mkn is slighly better that  the kmn implementation.

The worst case scenarios, instead, do the opposite: they privilegiate columns over rows, resulting in a better performance if the matrix was stored column-wise, like in Fortran, but leading to disastruous results in C, where the rowmajor order is preferable.

To verificate our assumption, we checked the number of cache misses for both the best and the worst implementation, resulting in the following table:

\section{Performance of n,k,m implementations with different sizes}
The results and conclusions presented in the previous section held keeping in mind that the matrices we tested were square ones, where $n=m=k$. We repeated the same tests for some odd-sized matrices, i.e. where one of the value was kept very low. The other two values were raised accordingly, in order to maintain the same memory footprint.The results we came up with are really interesting, and are presented in the three Figures ABC. The smallest valuse has always been kept to four elements.

//ABC

For small m and n the change is almost irrelevant. Surprisingly, for small ks, there is a drop in performance of the ones that before where the most performant results: mkn and kmn. This happens because 

//image/scheme to explain

\section{Blocked Matrix performance}
We recall that the blocked version, introduced in section //TODO, uses the best algorithm for naive multiplication, mkn, in its inner loop. We experimented the block size against the MFLOPs generated for a generally big square matrix (with 2048kB memory footprint). The results are reported in Figure //TODO (green line). The performances of the algorithm increased at the rising of the block size. This means that the optimum is the highes possible value of the block size, i.e. the matrix itself. In conclusion, the blocking algorithm is useless, because it will be always beaten by the nkm algorithm. 

To find an approximate optimum for the block size, it was necessary to substitute the algorithm in the inner loop with the worst one (knm). Repeating the test, the data showed a plateau curve, having its maximum in the range 27-37. After 40, the performances start to decrease, because the L1d cache starts to fill up. At 40, in fact, we have an estimated data cache size of $3 \cdot 40^2 \cdot 8 = 37.5kB$, which is sligthly higher than the l1d cache size ($32kB$) due to the prefetching effect.  



