The matrix multiplication is a critical operation in computer science, both because it is the base of several practical problems and because it is still relatively expensive. \\
This operation, in fact, is really the fundamental brick of several interesting problems, from the resolution of big linear systems, often applied to physics, to resolution of discrete Fourier transforms, used for example to DNA splicing problems.
On the main challenges of this operation is not related to the pure computational power, but with the amount of data that it requires: nowadays, as CPU/GPU evolved and still evolves faster than the memory chips, the real challenge is not to compute the result in a short time, but making the computational unit able to produce that result. In other words, it is more expensive bringing the right data at the right time, to the right chip, than to actually produce the result. \\
Algorithms in general nowadays, are so much affected by this behaviour, that the fastest ones are the algorithms that make a better usage of the caches.
From this point of view, matrix multiplication is considered a good example of what a typical algorithm should look like these days. It is therefore used as a base to test the ability of high performances computers to deliver outcomes. 
\\\\
\section{Loop Optimization Techniques}
Since scientific computing often requires the looping over large data sets there has been developed multiple optimization strategies that targets different areas. These areas are:
\\\\
\textbf{Spatial locality}\\
Reuse all the data fetched in a single cache line. Optimization techniques that leads to this is blocking.
\\\\
\textbf{Temporal locality}
Reuse data in previously fetched cache lines. Optimization techniques that leads to this is blocking.
\\\\
\textbf{loop bookkeeping}\\ 
This is the overhead of running the loop in the first place. This happens when the loop only does a small amount of computation. Techniques that addresses this are loop unrolling and loop fussion. 
\\\\
\textbf{Vectorization}\\
Modern CPU's contain units that can do floating point operations on more than one value at a time. If a loop is organised in a proper way and if the results calculated in the loop is not dependent upon each other it can be recognized by the compiler and assembler code will be generated that takes advantage of these units. Loop fission can help the compiler to do this. 
\\\\
Since loop blocking will be used later in this report we will cover it in more detail here.
\section{Loop Blocking}
A common technique to improve temporal and spatial locality at the cost of loop overhead is blocking. This technique takes a multi dimensional array and splits the computation on it up into smaller blocks. The size of the block is chosen to fit inside the cache size. This ensures that the cache is only filled with data that will be used in the current computations and later implementations. Blocking is implemented by having a set of loops that iterates over the blocks and then have a second set of loops inside the first set that iterates over each element. An implementation of this that calculates the transpose of matrix in C is shown below: 
\begin{lstlisting}
for(int i = 0 ; i < n ; n += nbi)
	for( int j = 0 ; j < n ; j++)
		for( int k = 0 ; k < min(n-i, nbi) ; k++)
			A[j][i+k] = B[i+k][j]
\end{lstlisting}
The above loop accesses array A in a column wise order which in C usually results in many cache misses but because this is only done for as much memory as there can be in the cache it means that the data that has been fetched once in one cache page can be used again in a later iteration of the loop.
\\\\
Blocking is a powerful technique to improve memory use and data reuse but also has its disadvantages. The optimal block size is dependent on the CPU the code is being run on and on what data the algorithm is used on. It is therefore a good idea to have parameters to specify the block size so the implementation can be ported to different architectures more easily.